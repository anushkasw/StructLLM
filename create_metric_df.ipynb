{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c4bd28-1a80-4129-9b05-9fd637ca7036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65ad841-9cf0-4bc9-8c56-560be02e6f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rc_type = 'JRE'\n",
    "res_path = \"./Analysis_extended/Results\"\n",
    "threshold = 0.95\n",
    "metrics = ['ts', 'cs', 'us']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6689e1-1706-4128-810a-ee7064631550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = ['NYT10', 'crossRE', 'tacred_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f941a444-b1d9-45fb-9487-0d841934d3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_mapping = {\n",
    "    \"OpenAI/gpt-4o\": \"GPT\",\n",
    "    \"openchat/openchat_3.5\": \"OpenChat\",\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\": \"Llama\",\n",
    "    \"mistralai/Mistral-Nemo-Instruct-2407\": \"Mistral\",\n",
    "    \"google/gemma-2-9b-it\": \"Gemma\",\n",
    "    \"PLM/UniRel\": \"UniRel\",\n",
    "    \"PLM/RIFRE\": \"RIFRE\",\n",
    "    \"PLM/TDEER\": \"TDEER\",\n",
    "    \"PLM/SPN4RE\": \"SPN4RE\",\n",
    "}\n",
    "plm_models = [\"PLM/RIFRE\", \"PLM/SPN4RE\", \"PLM/TDEER\"]\n",
    "llm_models = [\"openchat/openchat_3.5\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "              \"mistralai/Mistral-Nemo-Instruct-2407\", \"google/gemma-2-9b-it\",\n",
    "             \"OpenAI/gpt-4o\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b043ced-a386-44fb-aa89-3fe6e08ed012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(exp, file):\n",
    "    parts = file.parts\n",
    "    struct, parser, prompt, demo, seed, k = None, None, None, None, None, None\n",
    "    if exp=='plm':\n",
    "        seed = file.parts[-1].split('.')[0][-1]\n",
    "    elif exp=='1stage':\n",
    "        prompt = parts[-1].split('_')[-1].split('-')[0]\n",
    "        k = int(parts[-1].split('-')[-1].split('.')[0])\n",
    "        seed = parts[-2].split('-')[-1]\n",
    "        demo = parts[-3]\n",
    "    elif exp=='structure_extract':\n",
    "        prompt = parts[-1].split('_')[-1].split('-')[1]\n",
    "        struct = parts[-1].split('_')[-1].split('-')[0]\n",
    "        parser = parts[-1].split('_')[1]\n",
    "        k = int(parts[-1].split('_')[-1].split('-')[-1].split('.')[0])\n",
    "        seed = parts[-2].split('-')[-1]\n",
    "        demo = parts[-3]\n",
    "    else:\n",
    "        prompt = parts[-1].split('_')[1].split('-')[0]\n",
    "        k = int(parts[-1].split('_')[-1].split('-')[-1].split('.')[0])\n",
    "        seed = parts[-2].split('-')[-1]\n",
    "        demo = parts[-3]\n",
    "    return struct, prompt, seed, demo, parser, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddb4b9-29ad-4fa7-afef-3ff99522cb89",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Aggregate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a50d8a-f506-4aff-85d0-d1bcfda50153",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2stage\n",
      "structure_extract\n"
     ]
    }
   ],
   "source": [
    "for exp in ['2stage', 'structure_extract', 'plm']:\n",
    "    print(exp)\n",
    "    df = pd.DataFrame()\n",
    "    if exp=='plm':\n",
    "        models = plm_models\n",
    "    else:\n",
    "        models = llm_models\n",
    "    for metric in metrics:\n",
    "        # print(metric)\n",
    "        records = []\n",
    "        for data in ['crossRE', 'NYT10', 'tacred_new']:\n",
    "            # print(data)\n",
    "            for model in models:\n",
    "                # print(model)\n",
    "                model_ = model_mapping[model]\n",
    "\n",
    "                files = list(Path(f'{res_path}/{rc_type}/{exp}/{data}/{model}'\n",
    "                                 ).rglob('*.jsonl'))\n",
    "\n",
    "                for file in files:\n",
    "                    is_valid_file = (\n",
    "                        file.name.startswith(f'{metric}_') if metric in ['cs', 'us', 'ts', 'fs']\n",
    "                        else file.name.startswith('prf_')\n",
    "                    )\n",
    "                    \n",
    "                    if not is_valid_file:\n",
    "                        continue\n",
    "                        \n",
    "                    struct, prompt, seed, demo, parser, k = get_info(exp, file)                    \n",
    "                    with open(file, \"r\") as f:\n",
    "                        for line in f.read().splitlines():\n",
    "                            res_dict = json.loads(line)\n",
    "\n",
    "                    norm = []\n",
    "                    for key, val in res_dict.items():\n",
    "                        if metric in ['ts', 'fs']:\n",
    "                            score = val\n",
    "                            norm.append(score)\n",
    "                        elif metric in ['p', 'r', 'f']:\n",
    "                            if metric in val:\n",
    "                                score = val[metric]\n",
    "                                norm.append(score)\n",
    "                        else:\n",
    "                            score = val[f'{metric}_{threshold}']\n",
    "                            norm.append(score)\n",
    "\n",
    "                    records.append({'model':f'{model_}_{struct}' if struct else f'{model_}_{exp}', 'parser': parser,\n",
    "                                    'dataset':data, 'prompt':prompt,\n",
    "                                   'seed':seed,\n",
    "                                   metric:(np.mean(norm)*100)})     \n",
    "        df_llm = pd.DataFrame(records)\n",
    "        if len(df)==0:\n",
    "            df = df_llm.copy()\n",
    "        else:\n",
    "            df = pd.merge(df, df_llm, how='left', on=['model', 'dataset', 'prompt', 'seed', 'parser'])\n",
    "    if exp!=\"plm\":\n",
    "        df.to_csv(f'./eval_csvs/zero_metric_{exp}_{threshold}.csv', index=False)\n",
    "    else:\n",
    "        df.to_csv(f'./eval_csvs/zero_metric_{exp}.csv', index=False)\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-1.3",
   "language": "python",
   "name": "nlp-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
